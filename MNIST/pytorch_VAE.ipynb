{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79c0868-b733-422a-911d-299bea6931ce",
   "metadata": {},
   "source": [
    "# pytorch_VAE.ipynb\n",
    "# WESmith 07/15/23\n",
    "## Variational Autoencoder (VAE)\n",
    "## reference:\n",
    "## https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L17/1_VAE_mnist_sigmoid_mse.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871d603-282a-4b7b-9ed6-d314451e4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc211a8f-8d30-4695-b697-e6dc0d4edae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f'cuda:{CUDA_DEVICE_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229ae69-7e5a-4cd4-85bf-4695635ad19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d7589-54d0-4cc8-b594-be4f30b25aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size       = 64\n",
    "lr               = 1e-3\n",
    "decay            = 1e-5\n",
    "data_dir         = 'data'\n",
    "model_path       = 'results/model_VAE.pth'\n",
    "optimizer_path   = 'results/optimizer_VAE.pth'\n",
    "model_path_2     = 'results/model_VAE_2.pth'      # WS mods to model\n",
    "optimizer_path_2 = 'results/optimizer_VAE_2.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b5902-eadb-4940-a50b-10cfd49f5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6f24e-d179-431d-90d4-b9ec7c05b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5d910-8d57-48c1-8d60-09d44ef638d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a data batch for demonstrations\n",
    "dataiter = iter(data_loader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17460ad6-cfe8-4796-a87f-bf051c153880",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e776578-026c-44cc-9c47-860a8de141b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "\n",
    "class Trim(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :28, :28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332e7fb-b689-4739-b031-0da5f16d2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.Flatten(),)    \n",
    "\n",
    "        self.z_mean    = nn.Linear(3136, 2)\n",
    "        self.z_log_var = nn.Linear(3136, 2)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "                nn.Linear(2, 3136),\n",
    "                Reshape(-1, 64, 7, 7),\n",
    "                nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),                \n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0),                \n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0), \n",
    "                Trim(),  # 1x29x29 -> 1x28x28\n",
    "                nn.Sigmoid())\n",
    "        \n",
    "    def reparameterize(self, z_mu, z_log_var):\n",
    "        #eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.get_device())\n",
    "        eps = torch.randn(z_mu.size(0), z_mu.size(1))\n",
    "        z = z_mu + eps * torch.exp(z_log_var/2.) \n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, z_mean, z_log_var, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9473f1-13ce-42f4-b47f-996ef6029425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE2(nn.Module):  # WS mod to increase autoencoding layer size\n",
    "\n",
    "    def __init__(self, encode_size):  # WS mod\n",
    "        super().__init__()\n",
    "        self.encode_size = encode_size  # WS mod\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.Flatten(),)    \n",
    "\n",
    "        self.z_mean    = nn.Linear(3136, encode_size)  # WS mod\n",
    "        self.z_log_var = nn.Linear(3136, encode_size)  # WS mod\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "                nn.Linear(encode_size, 3136),  # WS mod\n",
    "                Reshape(-1, 64, 7, 7),\n",
    "                nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),                \n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0),                \n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0), \n",
    "                Trim(),  # 1x29x29 -> 1x28x28\n",
    "                nn.Sigmoid())\n",
    "\n",
    "    def reparameterize(self, z_mu, z_log_var):\n",
    "        #eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.get_device())\n",
    "        eps = torch.randn(z_mu.size(0), z_mu.size(1))\n",
    "        z = z_mu + eps * torch.exp(z_log_var/2.) \n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, z_mean, z_log_var, decoded    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0462d82b-1449-479a-b8b8-bcc200d7237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_loader, n_epochs=1, save=False):\n",
    "    # 'save' is a tuple of (model_path, optimizer_path), default=False\n",
    "    count = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        for (img, _) in data_loader:\n",
    "\n",
    "            encoded, z_mean, z_log_var, decoded = model(img)\n",
    "\n",
    "            # total loss = reconstruction loss + KL divergence\n",
    "            #kl_divergence = (0.5 * (z_mean**2 + \n",
    "            #                        torch.exp(z_log_var) - z_log_var - 1)).sum()\n",
    "            kl_div = -0.5 * torch.sum(1 + z_log_var \n",
    "                                        - z_mean**2 \n",
    "                                        - torch.exp(z_log_var), \n",
    "                                          axis=1) # sum over latent dimension\n",
    "\n",
    "            batchsize = kl_div.size(0)\n",
    "            kl_div    = kl_div.mean() # average over batch dimension\n",
    "\n",
    "            pixel_term = loss_fn(decoded, img, reduction='none')\n",
    "            pixel_term = pixel_term.view(batchsize, -1).sum(axis=1) # sum over pixels\n",
    "            pixel_term = pixel_term.mean() # average over batch dimension\n",
    "\n",
    "            loss = pixel_term + kl_div\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if count % 100 == 0:\n",
    "                print(f'epoch {epoch + 1} batch {count} has loss {loss.item():.4f}')\n",
    "            count += 1\n",
    "\n",
    "        print(f'Epoch: {epoch + 1}, Loss: {loss.item():.4f}')\n",
    "        if save:\n",
    "            torch.save(model.state_dict(),     save[0])\n",
    "            torch.save(optimizer.state_dict(), save[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed30a1ec-68e8-4caf-afdf-957457fa547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_or_load(model, optimizer, load, loss_fn=None, data_loader=None, n_epochs=1, force_train=False):\n",
    "    # 'load' is a tuple of (model_path, optimizer_path)\n",
    "    # if 'force_train' is True, the model will be trained regardless of saved training\n",
    "    # this is useful if additional training is desired of a saved model\n",
    "    # - first time force_train=False to either read in an existing training or start a new training\n",
    "    # - subsequent calls have force_train=True to further train the model\n",
    "    if os.path.isfile(load[0]) and os.path.isfile(load[1]) and not force_train:\n",
    "        print(f'reading model from {load}')\n",
    "        model_state_dict = torch.load(load[0])\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        optimizer_state_dict = torch.load(load[1])\n",
    "        optimizer.load_state_dict(optimizer_state_dict)\n",
    "    else:\n",
    "        print(f'training model, will be saved in {load}')\n",
    "        train(model, optimizer, loss_fn, data_loader, n_epochs=n_epochs, save=load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551b13d-187c-48d6-b091-f09bf1ac88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_loss(model, data):\n",
    "    '''convenience function to look at loss from a batch of data for a given model'''\n",
    "    encoded, z_mean, z_log_var, decoded = model(data)\n",
    "    print(f'LOSS over batch size of {data.shape[0]}: {loss_fn(data, decoded):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15980a1f-ba11-4078-8729-049a7b22ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_decoded(model, images, nr=6, nc=6, wid=12, hei=12):\n",
    "    encoded, z_mean, z_log_var, decoded = model(images)\n",
    "    plt.figure(figsize=(wid, hei))\n",
    "    plt.gray()\n",
    "    count = 1\n",
    "    for i, j in zip(images, decoded):\n",
    "        if count >= nr * nc: break\n",
    "        plt.subplot(nr, nc, count)\n",
    "        # item is (1, 28, 28) with singleton from data_loader\n",
    "        plt.imshow(i[0])\n",
    "        plt.subplot(nr, nc, count + 1)\n",
    "        plt.imshow(j[0].detach().numpy()) # also a singleton, so take [0]\n",
    "        count += 2\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'ENCODING DIMENSION: {encoded.shape[1]}', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa94d6ee-4d71-41b7-87fb-e859415cd79b",
   "metadata": {},
   "source": [
    "## ORIGINAL MODEL WITH ENCODING SIZE FIXED AT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182961f-f8b7-4ce0-9ce6-c2979fe57328",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path     = 'results/model_VAE.pth'\n",
    "optimizer_path = 'results/optimizer_VAE.pth'\n",
    "model          = VAE()\n",
    "loss_fn        = F.mse_loss\n",
    "optimizer      = optim.Adam(model.parameters(), lr=lr, weight_decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d19b7ff-c85b-4949-bb88-b2f7081e50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(k.numel() for k in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e75aac1-1cfe-43eb-914f-8b1492d9ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_loss(model, images)  # loss before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234ed45-ef6e-48a3-bf1f-445b964f8dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_or_load(model, optimizer, (model_path, optimizer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d100f-2034-45a8-85a3-ec97cb8e6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_loss(model, images)  # loss after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a2c25-9b34-4e1c-9b94-21d38fef39fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c07544-af4a-4e63-bb58-44898aff5470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(model, images.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354b2d9-57bf-4f4e-80ba-4ff707ddcd7f",
   "metadata": {},
   "source": [
    "## NEW MODEL WITH ENCODING SIZE 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2ba9a-d152-4664-8a90-bc722271c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_size      = 8  # was 2 in original model\n",
    "model_path_2     = f'results/model_VAE_{encode_size}.pth'      # WS mods to model\n",
    "optimizer_path_2 = f'results/optimizer_VAE_{encode_size}.pth'\n",
    "model2           = VAE2(encode_size)\n",
    "optimizer2       = optim.Adam(model2.parameters(), lr=lr, weight_decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a654f-f65b-4041-9ea9-a5b14b072754",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_2, optimizer_path_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f6cf4-6ac0-49e5-9ff0-457fa6842f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(k.numel() for k in model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59b537-dd26-42fa-9820-0e2026d843d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_loss(model2, images) # loss before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76348332-b84b-431e-a4cb-2d0dba2afd6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load = (model_path_2, optimizer_path_2)\n",
    "train_or_load(model2, optimizer2, load, loss_fn=loss_fn, data_loader=data_loader, force_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd85a5-822a-4389-8b53-953fb52e7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_loss(model2, images) # loss after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a589967-adf0-4b95-a9da-43f25f65a52f",
   "metadata": {},
   "source": [
    "## NEW MODEL WITH ENCODING SIZE 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965f699-b141-4d85-aa04-2aec52c17d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_size      = 16  # was 2 in original model\n",
    "model_path_3     = f'results/model_VAE_{encode_size}.pth'      # WS mods to model\n",
    "optimizer_path_3 = f'results/optimizer_VAE_{encode_size}.pth'\n",
    "model3           = VAE2(encode_size)\n",
    "optimizer3       = optim.Adam(model3.parameters(), lr=lr, weight_decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648117f-c00b-4774-b2ae-fece496df5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_3, optimizer_path_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8e8f5-834a-47b0-a88e-89f9aa37ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(k.numel() for k in model3.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f782221-758b-427b-b87b-d2895a9ce165",
   "metadata": {},
   "outputs": [],
   "source": [
    "load = (model_path_3, optimizer_path_3)\n",
    "train_or_load(model3, optimizer3, load, loss_fn=loss_fn, data_loader=data_loader, force_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116efbb9-cbf4-430b-8525-7e8ab712ff15",
   "metadata": {},
   "source": [
    "## EXAMINE ORIGINAL, DECODED, ENCODED (FOR 2D ENCODING) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70bdc67-e1b7-4bd5-aa0a-3a7eb135c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_decoded(model, images, nr=6, nc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08ef98-b91b-4daa-ba15-b5284d673419",
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_decoded(model2, images, nc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ed78b-c552-4eb6-8dcb-f914abee7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_decoded(model3, images, nc=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e139b9-0414-4c21-81a8-873efbe525e1",
   "metadata": {},
   "source": [
    "## EXAMINE DECODED FROM LINEAR SCAN OF ENCODING \n",
    "### (only works for 2D encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cfd82-d89e-4c19-ab8d-343a59dd30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "nr, nc = (7, 7)\n",
    "lim = 2\n",
    "count = 1\n",
    "encoded, z_mean, z_log_var, decoded = model(images)\n",
    "if encoded.shape[1] != 2:\n",
    "    sys.exit(f'ERROR: this plot only works for an encoding dimension of 2')\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.gray()\n",
    "for y in np.linspace(lim, -lim, num=nr):\n",
    "    for x in np.linspace(-lim, lim, num=nc):\n",
    "        dd = model.decoder(torch.tensor((x, y), dtype=torch.float32))\n",
    "        plt.subplot(nr, nc, count)\n",
    "        txt = f'({x:.2f},{y:.2f})'\n",
    "        plt.imshow(dd[0][0].detach().numpy())\n",
    "        plt.text(0, 0, txt, va='top', color='white', fontsize=12)\n",
    "        count += 1\n",
    "plt.suptitle(f'ENCODING DIMENSION: {encoded.shape[1]}', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e50da-f474-4829-8d51-625ae66b7d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
