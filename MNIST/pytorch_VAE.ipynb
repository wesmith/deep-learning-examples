{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79c0868-b733-422a-911d-299bea6931ce",
   "metadata": {},
   "source": [
    "# pytorch_VAE.ipynb\n",
    "# WESmith 07/15/23\n",
    "## Variational Autoencoder (VAE)\n",
    "## reference:\n",
    "## https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L17/1_VAE_mnist_sigmoid_mse.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871d603-282a-4b7b-9ed6-d314451e4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc211a8f-8d30-4695-b697-e6dc0d4edae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f'cuda:{CUDA_DEVICE_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229ae69-7e5a-4cd4-85bf-4695635ad19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d7589-54d0-4cc8-b594-be4f30b25aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size       = 64\n",
    "lr               = 1e-3\n",
    "decay            = 1e-5\n",
    "data_dir         = 'data'\n",
    "model_path       = 'results/model_VAE.pth'\n",
    "optimizer_path   = 'results/optimizer_VAE.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b5902-eadb-4940-a50b-10cfd49f5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6f24e-d179-431d-90d4-b9ec7c05b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5d910-8d57-48c1-8d60-09d44ef638d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get example data\n",
    "dataiter = iter(data_loader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17460ad6-cfe8-4796-a87f-bf051c153880",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e776578-026c-44cc-9c47-860a8de141b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "\n",
    "class Trim(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :28, :28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332e7fb-b689-4739-b031-0da5f16d2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.Flatten(),)    \n",
    "\n",
    "        self.z_mean    = nn.Linear(3136, 2)\n",
    "        self.z_log_var = nn.Linear(3136, 2)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "                nn.Linear(2, 3136),\n",
    "                Reshape(-1, 64, 7, 7),\n",
    "                nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),                \n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0),                \n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0), \n",
    "                Trim(),  # 1x29x29 -> 1x28x28\n",
    "                nn.Sigmoid())\n",
    "        \n",
    "    def reparameterize(self, z_mu, z_log_var):\n",
    "        #eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.get_device())\n",
    "        eps = torch.randn(z_mu.size(0), z_mu.size(1))\n",
    "        z = z_mu + eps * torch.exp(z_log_var/2.) \n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, z_mean, z_log_var, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182961f-f8b7-4ce0-9ce6-c2979fe57328",
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = VAE()\n",
    "loss_fn   = F.mse_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d879f3a-dcfd-4947-bb8d-951839264a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for k in model.parameters():\n",
    "    count += k.numel()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f86c74-d2e3-46f6-854c-39a1ba4520bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the model outputs\n",
    "encoded, z_mean, z_log_var, decoded = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dce4c7-a274-49c6-96f4-c4cea1950682",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.shape, z_mean.shape, z_log_var.shape, decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f3519-2ec3-474c-b9a8-2ee09ae87e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(images, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef7ae5-637b-4489-b2cc-052b155f1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if previous training exists, if so, load, otherwise train\n",
    "if os.path.isfile(model_path) and os.path.isfile(optimizer_path):\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    optimizer_state_dict = torch.load(optimizer_path)\n",
    "    optimizer.load_state_dict(optimizer_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592b40e-a6e8-480e-b55e-acfbf97884a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "n_epochs = 1  # 1 epochs took 1m 25s to run on acer\n",
    "outputs= []\n",
    "for epoch in range(n_epochs):\n",
    "    for (img, _) in data_loader:\n",
    "\n",
    "        encoded, z_mean, z_log_var, decoded = model(img)\n",
    "\n",
    "        # total loss = reconstruction loss + KL divergence\n",
    "        #kl_divergence = (0.5 * (z_mean**2 + \n",
    "        #                        torch.exp(z_log_var) - z_log_var - 1)).sum()\n",
    "        kl_div = -0.5 * torch.sum(1 + z_log_var \n",
    "                                    - z_mean**2 \n",
    "                                    - torch.exp(z_log_var), \n",
    "                                      axis=1) # sum over latent dimension\n",
    "\n",
    "        batchsize = kl_div.size(0)\n",
    "        kl_div    = kl_div.mean() # average over batch dimension\n",
    "\n",
    "        pixel_term = loss_fn(decoded, img, reduction='none')\n",
    "        pixel_term = pixel_term.view(batchsize, -1).sum(axis=1) # sum over pixels\n",
    "        pixel_term = pixel_term.mean() # average over batch dimension\n",
    "\n",
    "        loss = pixel_term + kl_div\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))\n",
    "    torch.save(model.state_dict(),   model_path)     #'results/model.pth')\n",
    "    torch.save(optimizer.state_dict(), optimizer_path) #'results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3cc88-d82e-4e30-be4e-a7c8be0d05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded, z_mean, z_log_var, decoded = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906de5d2-f478-4acc-8d23-b8dffe22761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape, encoded.shape, z_mean.shape, z_log_var.shape, decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18081cbf-40bc-475d-a1e9-03db2e5f94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc  = 20\n",
    "lim = 2\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.gray()\n",
    "for i, item in enumerate(images):\n",
    "    if i >= nc: break\n",
    "    plt.subplot(3, nc, i+1)\n",
    "    #item = item.reshape(-1, 28, 28)\n",
    "    # item is (1, 28, 28) with singleton from data_loader\n",
    "    plt.imshow(item[0])\n",
    "\n",
    "for i, item in enumerate(decoded):\n",
    "    if i >= nc: break\n",
    "    plt.subplot(3, nc, nc+i+1)\n",
    "    #item = item.reshape(-1, 28, 28)\n",
    "    plt.imshow(item[0].detach().numpy()) # also a singleton, so take [0]\n",
    "    \n",
    "for i, item in enumerate(encoded):\n",
    "    if i >= nc: break\n",
    "    plt.subplot(3, nc, 2*nc+i+1)\n",
    "    val = item.detach().numpy()\n",
    "    plt.scatter(val[0], val[1])\n",
    "    plt.xlim(-lim, lim)\n",
    "    plt.ylim(-lim, lim)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cfd82-d89e-4c19-ab8d-343a59dd30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = (7, 7)\n",
    "lim = 2\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.gray()\n",
    "count = 1\n",
    "for y in np.linspace(lim, -lim, num=nr):\n",
    "    for x in np.linspace(-lim, lim, num=nc):\n",
    "        dd = model.decoder(torch.tensor((x, y), dtype=torch.float32))\n",
    "        plt.subplot(nr, nc, count)\n",
    "        txt = f'({x:.2f},{y:.2f})'\n",
    "        plt.imshow(dd[0][0].detach().numpy())\n",
    "        plt.text(0, 0, txt, va='top', color='white', fontsize=12)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd210bbd-6976-48df-9a4b-3e1cf192efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(2, -2, num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132a909-74ff-4db7-b96f-15e4d8c3d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = model.decoder(torch.tensor((-1, 0), dtype=torch.float32))\n",
    "plt.imshow(dd[0][0].detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfad42-ef29-491e-89fc-a3d5c69b9ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
